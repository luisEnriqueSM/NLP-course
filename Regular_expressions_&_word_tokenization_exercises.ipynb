{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18563f8a-1089-4af4-911d-8459cbd871f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luisenrique/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import regexp_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f983e0-1c85-44c7-a330-de271c81dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"Natural Language Processing (NLP) is a fascinating field! It combines linguistics, computer science, and artificial intelligence. How can machines understand human language? That's the challenge!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8889e141-fa06-4db8-8e9c-c5171e609203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) is a fascinating field!', 'It combines linguistics, computer science, and artificial intelligence.', 'How can machines understand human language?', \"That's the challenge!\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5b9078-947f-42cd-9847-4427669899a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '!', 'It', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'How', 'can', 'machines', 'understand', 'human', 'language', '?', 'That', \"'s\", 'the', 'challenge', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(frase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df763d-e1ec-49c7-8854-248942de9231",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Contar palabras largas\n",
    "Objetivo:\n",
    "\n",
    "Usa word_tokenize para separar las palabras de un p치rrafo.\n",
    "\n",
    "Cuenta cu치ntas tienen 7 letras o m치s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c925f7-c934-49a0-933a-c09f0ce99e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Machine learning algorithms are powerful tools for analyzing large datasets and making predictions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65bafe12-dbbd-4daf-88c3-7f731858bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine:7', 'learning:8', 'algorithms:10', 'powerful:8', 'analyzing:9', 'datasets:8', 'predictions:11']\n"
     ]
    }
   ],
   "source": [
    "words7lenght = [i + \":\" + str(len(i)) for i in word_tokenize(texto) if len(i) >= 7]\n",
    "print(words7lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bb048-a77d-40f6-94aa-20914ab2f53d",
   "metadata": {},
   "source": [
    "##  Ejercicio 2: Encontrar preguntas\n",
    "Objetivo:\n",
    "\n",
    "Usa sent_tokenize para dividir un texto con varias oraciones.\n",
    "\n",
    "Usa una expresi칩n regular o filtrado para imprimir solo las oraciones que sean preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b392274e-1023-465b-b28d-1ea8f3401800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2238aee1-78aa-4992-aa93-18529339d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsAndanswers = \"What is Natural Language Processing? It is a subfield of artificial intelligence. How does it work? Many people want to learn it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cbbf5b66-a653-439d-a7b6-6117ab525349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sent_tokenize(questionsAndanswers) \u001b[38;5;28;01mif\u001b[39;00m regexp_tokenize(i, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/regexp.py:216\u001b[0m, in \u001b[0;36mregexp_tokenize\u001b[0;34m(text, pattern, gaps, discard_empty, flags)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*.  See :class:`.RegexpTokenizer`\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03mfor descriptions of the arguments.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(pattern, gaps, discard_empty, flags)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/regexp.py:123\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_regexp()\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# If our regexp matches gaps, use re.split:\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gaps:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/regexp.py:120\u001b[0m, in \u001b[0;36mRegexpTokenizer._check_regexp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_regexp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_regexp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_regexp \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pattern, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/__init__.py:228\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(pattern, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/__init__.py:307\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m--> 307\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m DEBUG:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/_compiler.py:745\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[1;32m    744\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m--> 745\u001b[0m     p \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse(p, flags)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/_parser.py:979\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    976\u001b[0m state\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m flags\n\u001b[1;32m    977\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m--> 979\u001b[0m p \u001b[38;5;241m=\u001b[39m _parse_sub(source, state, flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    980\u001b[0m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m fix_flags(\u001b[38;5;28mstr\u001b[39m, p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags)\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/_parser.py:460\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    458\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    461\u001b[0m                        \u001b[38;5;129;01mnot\u001b[39;00m nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items))\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/re/_parser.py:687\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    685\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item \u001b[38;5;129;01mor\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m AT:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnothing to repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    688\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _REPEATCODES:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    691\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n",
      "\u001b[0;31merror\u001b[0m: nothing to repeat at position 0"
     ]
    }
   ],
   "source": [
    "[i for i in sent_tokenize(questionsAndanswers) if regexp_tokenize(i, r\"?\\w+\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cbfc2f53-1ff1-4a94-b951-3cc1711dc5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionsAndanswers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928aca26-c1e5-4ee4-aa6f-d90b39acdacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
